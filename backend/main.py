from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Literal, Dict
from pydantic import BaseModel
from pathlib import Path

from settings import (
    PROVIDER,
    FRONTEND_ORIGIN,
    OPENAI_API_KEY,
    GROQ_API_KEY,
    RESUME_TEXT,
    OPENAI_MODEL,
)
from providers.mock_provider import MockProvider
from providers.openai_provider import OpenAIProvider
from providers.groq_provider import GroqProvider
from rag.index import ResumeRAG

app = FastAPI()

# allow local React dev server
app.add_middleware(
    CORSMiddleware,
    allow_origins=[FRONTEND_ORIGIN],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Select provider
if PROVIDER == "openai" and OPENAI_API_KEY:
    chat_provider = OpenAIProvider()
    print(
        f"[boot] provider={PROVIDER} openai={bool(OPENAI_API_KEY)} groq={bool(GROQ_API_KEY)}"
    )
elif PROVIDER == "groq" and GROQ_API_KEY:
    chat_provider = GroqProvider()
    print(
        f"[boot] provider={PROVIDER} openai={bool(OPENAI_API_KEY)} groq={bool(GROQ_API_KEY)}"
    )
else:
    print("No valid API key found — using MockProvider for now.")
    chat_provider = MockProvider()


# Pydantic models for request validation
class Msg(BaseModel):
    role: Literal["user", "assistant", "system"]
    content: str


class ChatRequest(BaseModel):
    messages: List[Msg]  # we expect a short window of the recent chat


RAG_PATH = Path("data/resume.clean.txt")

if RESUME_TEXT:
    RAG_PATH.parent.mkdir(parents=True, exist_ok=True)
    RAG_PATH.write_text(RESUME_TEXT, encoding="utf-8")

rag = None
if RAG_PATH.exists():
    try:
        rag = ResumeRAG(RAG_PATH)
        n = rag.build()
        print(f"RAG index ready with {n} chunks from {RAG_PATH}")
    except Exception as e:
        print(f"RAG init failed: {e}")
else:
    print(f"No {RAG_PATH}; RAG disabled")


@app.post("/api/chat")
def chat(req: ChatRequest):
    """Return one assistant reply generated by the configured provider."""

    msgs: List[Dict[str, str]] = [m.model_dump() for m in req.messages]

    last_user = next((m["content"] for m in reversed(msgs) if m["role"] == "user"), "")
    context = ""
    if rag and last_user:
        top = rag.search(last_user, k=3)
        if top:
            context = "Relevant resume snippets:\n" + "\n".join(
                f"- {t}" for t, _ in top
            )

    reply = chat_provider.generate(msgs, context=context)
    return {"reply": reply}


@app.get("/api/health")
def health():
    return {"ok": True}


@app.get("/api/resume")
def resume():
    # temporary stub; we’ll swap this with your real data later
    return {
        "name": "Your Name",
        "title": "Software Engineer",
        "skills": ["Python", "FastAPI", "React", "PostgreSQL"],
        "projects": [
            {"name": "Project A", "desc": "What it does…", "link": "#"},
            {"name": "Project B", "desc": "What it does…", "link": "#"},
        ],
    }


@app.get("/")
def root():
    return {"message": "Backend is running!"}


@app.get("/api/debug/config")
def debug_config():
    return {
        "provider": PROVIDER,
        "has_openai_key": bool(OPENAI_API_KEY),
        "has_groq_key": bool(GROQ_API_KEY),
        "openai_model": OPENAI_MODEL,
    }


@app.get("/api/rag/reload")
def rag_reload():
    """Rebuild index after updating resume.clean.txt."""
    global rag
    if not RAG_PATH.exists():
        return {"ok": False, "error": f"Missing {RAG_PATH}"}
    try:
        if rag is None:
            rag = ResumeRAG(RAG_PATH)
        n = rag.build()
        return {"ok": True, "chunks": n}
    except Exception as e:
        return {"ok": False, "error": str(e)}


@app.get("/api/rag/debug")
def rag_debug(q: str = "", k: int = 3):
    """Return top-k snippets for a query (no model call)."""
    if not rag:
        return {"ok": False, "error": "RAG not initialized"}
    res = [{"text": t, "score": s} for t, s in rag.search(q or "", k=k)]
    return {"ok": True, "results": res}
